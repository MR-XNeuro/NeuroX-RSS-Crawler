import os
from dotenv import load_dotenv
load_dotenv()

import requests
from bs4 import XMLParsedAsHTMLWarning
import warnings
warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)
from bs4 import BeautifulSoup
import hashlib
import random
import redis
from redis.connection import SSLConnection
from datetime import datetime, timedelta, timezone

# === ØªÙ†Ø¸ÛŒÙ…Ø§Øª ===
BACKENDLESS_APP_ID = os.getenv("BACKENDLESS_APP_ID")
BACKENDLESS_API_KEY = os.getenv("BACKENDLESS_API_KEY")
BACKENDLESS_API_URL = os.getenv("BACKENDLESS_API_URL")
BACKENDLESS_TABLE = "Posts"
API_URL = f"{BACKENDLESS_API_URL}/{BACKENDLESS_APP_ID}/{BACKENDLESS_API_KEY}/data/{BACKENDLESS_TABLE}"

TARGET_SITES_FILE = "target_sites.txt"
PLATFORMS = ["WordPress", "Blogspot", "Tumblr", "X"]

# --- Ø§ØªØµØ§Ù„ Ø¨Ù‡ Redis Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² REDIS_URL Ø§Ø² .env ---
REDIS_URL = os.getenv("REDIS_URL")
redis_client = redis.Redis.from_url(REDIS_URL, connection_class=SSLConnection)

# --- Ù„ÙˆØ¯ Ú©Ø±Ø¯Ù† Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù‡Ø¯Ù ---
def load_target_sites():
    try:
        with open(TARGET_SITES_FILE, "r", encoding="utf-8") as f:
            return [line.strip() for line in f if line.strip()]
    except Exception as e:
        print("âš ï¸ Failed to load target sites:", e)
        return []

# --- Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø§Ø² Ø³Ø§ÛŒØª ---
def extract_text_from_site(url):
    try:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")
        paragraphs = soup.find_all("p")
        text = "\n".join(p.get_text() for p in paragraphs if len(p.get_text()) > 80)
        return text.strip()
    except Exception as e:
        print(f"Error scraping {url}: {e}")
        return None

# --- Ù„ÙˆØ¯ ØªØ¨Ù„ÛŒØºØ§Øª ---
def load_promos(file_path="promo_texts.txt"):
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            lines = [line.strip() for line in f if line.strip()]
            return lines
    except Exception as e:
        print("âš ï¸ Failed to load promos:", e)
        return []

PROMO_LINES = load_promos()

# --- ØªÙˆÙ„ÛŒØ¯ Ù¾Ø³Øª Ø¨Ø±Ø§ÛŒ Backendless ---
def generate_post(text, source_url):
    promo = random.choice(PROMO_LINES) if PROMO_LINES else ""
    platform = random.choice(PLATFORMS)
    now = datetime.now(timezone.utc)
    return {
        "title": "Betting Risk Exposed",
        "description": text + "\n\n" + promo,
        "imageUrl": "",
        "sourceUrl": source_url,
        "targetPlatform": platform,
        "scheduledAt": (now + timedelta(minutes=random.randint(5, 60))).strftime("%Y-%m-%d %H:%M:%S"),
        "status": "scheduled",
        "createdAt": now.strftime("%Y-%m-%d %H:%M:%S"),
        "updatedAt": now.strftime("%Y-%m-%d %H:%M:%S")
    }

# --- Ø§Ø±Ø³Ø§Ù„ Ù¾Ø³Øª ---
def post_to_backendless(data):
    try:
        r = requests.post(API_URL, json=data)
        r.raise_for_status()
        print("âœ… Sent to Backendless:", data["title"], "â†’", data["targetPlatform"])
    except Exception as e:
        print("âŒ Failed to send post:", e)

# --- Ø§Ø¬Ø±Ø§ÛŒ Ø§ØµÙ„ÛŒ ---
    TARGET_SITES = load_target_sites()
    for site in TARGET_SITES:
        print("ğŸ” Scraping:", site)
        text = extract_text_from_site(site)
        if not text:
            continue
        content_hash = hashlib.sha256(text.encode()).hexdigest()
        if redis_client.sismember("seen_hashes", content_hash):
            print("â­ï¸ Duplicate content. Skipping.")
            continue
        post = generate_post(text, site)
        post_to_backendless(post)
        redis_client.sadd("seen_hashes", content_hash)


import sys

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print("âŒ Unhandled exception:", e)
        sys.exit(1)
    sys.exit(0)
